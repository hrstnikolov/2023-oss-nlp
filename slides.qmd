---
title: "Going far with open source tools in NLP"
subtitle: "Krum Arnaudov, Data Scientist at Financial Times, 2023"
format:
  revealjs: 
    incremental: true
    slide-number: true
    theme: serif
    preview-links: auto
    logo: images/nlp_tools.jpeg
    css: styles.css
    footer: SoftUni AI conference
resources:
  - slides.pdf
execute:
  eval: false
---

## About me

- Joined Financial Times as Data Scientist in 2021
- Past jobs as DS at Amplify Analytics, before that Account & Ops Management at HPI
- ML knowledge largely self-taught via free online content + the AI specialisation at the SoftUni.
- Main hobbies: my kids, some music, local politics

## Topic today

- Work with a NLP dataset.
- Discuss central practical NLP concepts.
- Build a small app.

---


::: {style="margin-top: 3em"}
### Value in NLP can be created with Open Source Tools.
### Small scale is fine.
### Search is more important than chat. Still.
:::

## Why do I strongly dislike closed source

::: incremental
- **Not the spirit of ML.**
- **Not the spirit of SW.**
- Danger of knowledge 
:::

## Also

![](images/open-vs-closed-source.png){fig-align="center" width=110% height=110%}

::: footer
Source: https://twitter.com/julien_c/status/1648738233696632832/photo/1
:::

## (Another) Revolution in NLP

::: incremental
- Generative foundational models - GPT-3.5/GPT-4, BARD, LlaMa
- Focus on text generation/human interactions
- Semantic search
- Vector storage / VectorDBs - Pinecone, Weaviate, ChromaDB, etc...
- Amazing open source tools - Huggingface, LangChain...
:::

## What is a open source language model?

- [Definition](https://opensource.org/osd/)
- Open Source is a scale:
  - [Sentence Transformers](https://github.com/UKPLab/sentence-transformers/blob/master/LICENSE) are open source (Apache 2.0 License)
  - Huggingface's BLOOM is [NOT open source](https://bigscience.huggingface.co/blog/the-bigscience-rail-license)
  - But OpenAI's GPT-3.5/4 is as closed source as it gets
    - WTF is that - https://openai.com/policies

## Open Source LLM Landscape

![](images/os-recent-landscape.jpeg)

::: footer
Source: https://twitter.com/theaievangelist/status/1645809824314298368/photo/1
:::

## Open Source vs. Propriatary 

- ChatGPT is so cool!!!
  - quasi-monopoly in text generation/interactions
  - rates changes risk
  - public to private knowledge (SOF)

## Open Source vs. Propriatary

- Huggingface 
- Academia 
- The ML Community TM

## Open?AI

My data, my kingdom.

## Step 1 - Data

- [Recipe Box](https://eightportions.com/datasets/Recipes/) dataset
- Example:
```{python}

  "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi": {
    "instructions": "Toss ingredients lightly and spoon into a buttered baking dish. ...",
    "ingredients": [
      "1/2 cup celery, finely chopped",
      "1 small green pepper finely chopped",
      "..."
    ],
    "title": "Grammie Hamblet's Deviled Crab",
    "picture_link": null
  }

```

## Data Preprocessing

```{python}
#| echo: true

# imports and json files parsing skipped
# see data.preprocess_data.combine_json_to_dataframe.py

# Combine the data from the three JSON files
data = {**fn_data, **epi_data, **ar_data}

# Convert the data to a dataframe
df = pd.DataFrame.from_dict(data, orient='index')

# Add a new column with the concatenated text
df['full_text'] = ('Recipe title: ' + 
                        df['title'] + 
                        '. Ingredients: ' + 
                        df['ingredients'].apply(lambda x: '; '.join(x)) + 
                        '. Instructions: ' + 
                        df['instructions'])

df = (df.
      # remove adds
      pipe(remove_advertisement).
      # drop the picture_link column
      drop(['picture_link'], axis = 1).
      # give a num_words estimation
      assign(num_words = lambda d: d['full_text'].str.split().str.len()).
      # drop short articles
      loc[lambda d: d['num_words'] > num_words_cutoff]
)

```

## Data Preprocessing

```{.python code-line-numbers="10-18|19-22|23-24|25-26"}
#| echo: true

# imports and json files parsing skipped
# see data.preprocess_data.combine_json_to_dataframe.py

# Combine the data from the three JSON files
data = {**fn_data, **epi_data, **ar_data}

# Convert the data to a dataframe
df = pd.DataFrame.from_dict(data, orient='index')

# Add a new column with the concatenated text
df['full_text'] = ('Recipe title: ' + 
                        df['title'] + 
                        '. Ingredients: ' + 
                        df['ingredients'].apply(lambda x: '; '.join(x)) + 
                        '. Instructions: ' + 
                        df['instructions'])

df = (df.
      # remove adds
      pipe(remove_advertisement).
      # drop the picture_link column
      drop(['picture_link'], axis = 1).
      # give a num_words estimation
      assign(num_words = lambda d: d['full_text'].str.split().str.len()).
      # drop short articles
      loc[lambda d: d['num_words'] > num_words_cutoff]
)

```

## Step 2 - Document Embeddings

::: incremental
::: {style="margin-top: 5em;" .fragment .fade-in}
- What is a `document`?
- What is an `embedding`?
:::
:::


## Document Embeddings Goal 

::: {style="margin-top: 5em;" }
### Find numerical representation of the documents such that semantically similar articles are close.
#### And semantically dissimilar articles - far. 
:::

## Document representations

::: {layout="[[-1], [1], [-1]]"}
![](/images/doc_representations.png){fig-align="center" width=110% height=110%}
:::

## Document representations

::: {layout="[[-1], [1], [-1]]"}
![](/images/doc_reps_movies.png){fig-align="center" width=110% height=110%}
:::


## Old School - Term frequency

::: {layout="[[-1], [1], [-1]]"}
![](/images/term_freq.png){fig-align="center"}
:::

## Old School - Inverse Document Frequency

::: {layout="[[-1], [1], [-1]]"}
![](/images/idf.png){fig-align="center"}
:::

## Old School - TF-IDF

::: {layout="[[-3, 45,-19, 16, -19], [100]]" layout-valign="top"}
![](/images/term_freq.png){fig-align="right"}

![](/images/idf.png){fig-align="left"}

![](/images/tf-idf.png)
:::

## Common Sparse Representation Issues

Frequently mentioned:
- Lexical gap: UK, United Kingdom, England
- Word order not preserved

Actual issues:
- Requires a “vocabulary” of size (n_training docs, nr_tokens_retained) in memory e.g. (50 000, 100 000)
- Huge vectors - tough for clustering tasks


## Old School - TF-IDF 

```{python}
#| echo: true
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

vectoriser = TfidfVectorizer(
    stop_words='english', # default is without it, but this decreases the dictionary size significantly
    min_df = 2, # Ignore terms that have a document frequency strictly lower than the given threshold. When float, proportion of docs.
    max_df = 0.95, # ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).
    ngram_range=(1,2), # uni and bi-grams
    max_features=30_000, # unigrams are ca. 22K, get top 8000 bigrams
    dtype=np.float32 # Reduces the size of the resulting array without much quality sacrifice, default is float64
)

embeddings = vectoriser.fit_transform(recipe_data.full_text)
```


## Dense document representations

``` tex
f(Text) -> Representation^n
```

- `n` dimensional representation
- Find function `f` such that semantically similar text is close

::: footer
Source: Nils Reimers, Introduction to Dense Text Representations
:::

## What does semantically similar mean?

![](/images/sem_similarity.png)

::: footer
Based on: Nils Reimers, Introduction to Dense Text Representations
:::

## Sentence Transformers

![](/images/sbert.png)

## Loss functions

![](https://huggingface.co/blog/assets/95_training_st_models/datasets_table.png)

## SentenceTransformers - Triplet loss example

```python
{'set':
 {'query': 'What can I do to get better grades?',
  'pos': 
['How do I improve my grades?'],
  'neg': 
['Why do I get bad grades even though I study a lot?',
   'How can I get better grades in maths?',
   'How serious is forging high school grades?']
}
}
```

![](/images/triplet_loss.png)

::: footer
Nils Reimers, Introduction to Dense Text Representations
:::

## Sentence Transformers - Choices

A family of language models finetuned to produce document\* embeddings

::: {style="text-align: center; margin-top: 1em"}
[Pretrained Sentence Transformers](https://www.sbert.net/docs/pretrained_models.html){preview-link="true" style="text-align: center"}
:::

::: footer
\*Up to 512 tokens (ca. 400 words)
:::

## Choices - MTEB

Massive Text Embedding Benchmark ([MTEB](https://huggingface.co/spaces/mteb/leaderboard){preview-link="true" style="text-align: center"})

![](/images/mteb_2.png)

## Sentence Transformers API

::: {style="text-align: center; margin-top: 4em"}
```{python}
#| echo: true
from sentence_transformers import SentenceTransformer

# download model
vectoriser = SentenceTransformer("all-MiniLM-L12-v2")
# ensure that the model vectorises up to 512 tokens
vectoriser.max_seq_length = 512

docs = [rec for rec in recipe_data.full_text]
embeddings = vectoriser.encode(docs, show_progress_bar=True)
```
:::

# How do you know if you've done a good job?

## Embeddings visualizations

1. Reduce dimensions to 2 (UMAP seems to be the best option, TruncatedSVD for sparse data)
2. Create an interactive plot:
  
  - good options - altair, bokeh, plotly

## Embeddings visualizations with Bulk

- [Bulk](https://github.com/koaning/bulk) - visualisation and initial data labelling in one.
- Works with images too!

::: {style="text-align: center; margin-top: 4em"}
```{python}
#| echo: true
!python -m bulk text data/bulk_st.csv
```
:::

## Simple Semantic Search

```{python}
def get_most_similar_doc(text, vectorized_corpus, original_data, vectoriser=vectoriser, top_n = 5):
    # encodes the input text
    new_doc_vector = vectoriser.encode(text)
    # calculates the cosine similarity between the new text embedding 
    # and all other vectors
    sim = cosine_similarity(X = vectorized_corpus, Y = new_doc_vector).flatten()
    # gets the indices of the top_n highest similarities
    argmax = np.argpartition(sim, -top_n)[-top_n:]
    # returns the original data filtered for the top results
    return(
        (original_data.
         iloc[argmax].
         # adds the cosine similarity as a column
         assign(similarity = sim[argmax]).
         loc[:, ['title', 'ingredients', 'similarity']].
         sort_values('similarity', ascending = False).
         reset_index(drop=True)
         )

    )
```

## Simple Semantic Search

```{python}
creme_brulee_recipe = [
    """Ingredients:
- 2 cups heavy cream
- 1 vanilla bean, split and scraped or 1 tsp vanilla extract
- 5 egg yolks
- 1/2 cup granulated sugar, plus more for caramelizing

Instructions: ... truncated
"""
]

get_most_similar_doc(text = creme_brulee_recipe, 
                     vectorized_corpus = embeddings, 
                     original_data = recipe_data,
                     top_n=10)

# Returns    
## 1) Crème Brûlée                     - Cosine Similarity = 0.870680
## 2) Chocolate Sprinkled Creme Brulee - Cosine Similarity = 0.822773
```

# Further know your dataset

## Topic modelling with BERTopic

- [BERTopic](https://maartengr.github.io/BERTopic/index.html) - like a lego for topic modelling
- Embrace modularity
![](https://maartengr.github.io/BERTopic/algorithm/default.svg){width=80% height=80%}
- Fast, when you already have the embeddings

## Topic modelling with BERTopic

```{python}
#| echo: true

vectoriser_model = CountVectorizer(stop_words="english")
hdbscan_model = HDBSCAN(min_cluster_size=10, 
                        min_samples = 1, # This to reduce outliers as much as possible
                        cluster_selection_epsilon = 0.1, # Reduce number of clusters
                        metric='euclidean', 
                        prediction_data=True)

sentence_model = SentenceTransformer("all-MiniLM-L12-v2")
sentence_model.max_seq_length = 512

representation_model = MaximalMarginalRelevance(diversity=0.2)

topic_model = BERTopic(
    vectorizer_model=vectoriser_model,
    hdbscan_model = hdbscan_model,
    min_topic_size=20,
    n_gram_range=(1, 2),
    embedding_model=sentence_model, 
    representation_model=representation_model
    )

topics, probs = topic_model.fit_transform(docs, embeddings)
```

## Topic modelling with BERTopic

![](/images/bertopic_intertopic_distance.mov)

# Annotate your dataset

## Some work - already done

- Use Bulk
- Use BERTopic
- Third option - annotation tools

## Annotation

Any process of adding metadata tags to your text data can be called `annotation`.

Different examples of annotation:

- Text classification
- Named entities
- Entity linking

## Annotation Tools

1. Paid tools:
    - [Prodigy](https://prodi.gy/) (by Explosion (the SpaCy maintainers)) - supports `active learning`
    - [TagTog](https://www.tagtog.com/-plans)
    - [LightTag](https://www.lighttag.io/pricing)

2. Open Source tools:
    - [Argilla](https://docs.argilla.io/en/latest/index.html) - New and quite powerful.
    - [Doccano](https://doccano.github.io/doccano/)
    - [Pigeon](https://github.com/agermanidis/pigeon) - Simplest of all

## Pigeon - annotate in Jupyter

```{.python}
annotations = annotate(
  recipe_data.full_text.sample(100),
  # 1 - Very Easy, 2 - Kinda Easy, 3 - Moderate to Hard, 4 - Hard
  options=['1', '2', '3', '4'],
  # The below is needed just because the recipes are long and tough to see.
  display_fn=lambda x: pprint(x)
)
```

# Few-shot learning with SetFit

## Zero-shot vs. Few-shot learning

![](/images/zero-shot.png){.absolute top=70 left=0 width="45%" height="30%"}

![](/images/few-shot.png){.absolute bottom=0 right=0 width="45%" height="45%"}

::: footer
Source: GPT-3 paper https://arxiv.org/pdf/2005.14165.pdf
:::

## Few-shot learning with SetFit

```{python}
# define the model
model_id = "sentence-transformers/all-MiniLM-L12-v2"
model = SetFitModel.from_pretrained(model_id)
model.model_body[0].max_seq_length = 512

# get the dataset - ca. 15 examples per 4 categories 
# 1 - easy to 4 - hard
annotated_df = pd.read_parquet("https://raw.githubusercontent.com/krumeto/oss_nlp_tools_demos/main/data/recipe_classes.parquet")
train_dataset = Dataset.from_pandas(annotated_df)

# Train the model
trainer = SetFitTrainer(
    model=model,
    train_dataset=train_dataset,
    loss_class=CosineSimilarityLoss,
    num_iterations=20,
    batch_size = 5, # Reduce the batch size due to memory issues
    column_mapping={"recipe": "text", "label": "label"},
)
trainer.train()

complicated_recipe = """Some complicated recipe"""

trainer.model.predict([complicated_recipe])
```

# Deploy

## Streamlit - why do I love it?

1. Do your usual flow
2. Sprinkle some [streamlit](https://docs.streamlit.io/) calls
    
    - Love the docs & blog

3. Get your app. 
4. Stakeholders and teammates love it!

## Steamlit notes

Utilize caching - `@st.cache_resource` and `@st.cache_data` and wrap in functions

```{.python}
@st.cache_resource
def load_model():
    model = SentenceTransformer("all-MiniLM-L12-v2")
    model.max_seq_length = 512
    return(model)
```

# Scale

## Indexes

Indexes have two main components:

- Storing docs + docs embeddings + metadata
- Scaled Search implementations:
  - KNN (also on GPU )
  - ANN (different algos like HNSW (Hierarchical Navigable Small World))

See e.g. this for index choices - https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index

## Vector Stores

Indexing implementations:
- ElasticSearch
- FAISS (Facebook, but open source)
- OpenSearch
- Chroma

Some paid ones with free tiers:
- Pinecone
- Qdrant

# A note on `LangChain`

## Summary

# Q&A