---
title: "Going far with open source tools in NLP"
subtitle: "Krum Arnaudov, Data Scientist at Financial Times, 2023"
format:
  revealjs: 
    incremental: true
    slide-number: true
    chalkboard: 
      buttons: false
    theme: serif
    preview-links: auto
    logo: images/nlp_tools.jpeg
    css: styles.css
    footer: sli.do AI-Conf
resources:
  - slides.pdf
execute:
  eval: false
---

## About me

::: {style="margin-top: 3em"}
- Joined Financial Times as Data Scientist in 2021
- Past jobs as DS at Amplify Analytics, before that Account & Ops Management at HPI
- ML knowledge largely self-taught via free online content + the AI specialisation at the SoftUni.
- Main hobbies: my kids, some music, local politics
:::

## Topic today

::: {style="margin-top: 3em"}
- Work with a NLP dataset.
- Discuss central practical NLP concepts.
- Build a small app.
- [Repo](https://github.com/krumeto/oss_nlp_tools_demos){preview-link="true"}
:::

## Before we start - Definition

::: {style="margin-top: 3em"}
*Open-source software (OSS) is computer software that is released under a license in which the copyright holder grants users the rights to use, study, change, and distribute the software and its source code to anyone and for any purpose.*
:::

::: footer
Source: Wikipedia
:::

## Before we start - Motivation

![](images/bulk_demo.mov)

## Step 1 - Data

- [Recipe Box](https://eightportions.com/datasets/Recipes/){preview-link="true"} dataset
- Data Example:
  ```{.python}

    "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi": {
      "instructions": "Toss ingredients lightly and spoon into a buttered baking dish. ...",
      "ingredients": [
        "1/2 cup celery, finely chopped",
        "1 small green pepper finely chopped",
        "..."
      ],
      "title": "Grammie Hamblet's Deviled Crab",
      "picture_link": null
    }

  ```

## Data Preprocessing

```{python}
#| echo: true

# imports and json files parsing skipped
# see data.preprocess_data.combine_json_to_dataframe.py

# Combine the data from the three JSON files
data = {**fn_data, **epi_data, **ar_data}

# Convert the data to a dataframe
df = pd.DataFrame.from_dict(data, orient='index')

# Add a new column with the concatenated text
df['full_text'] = ('Recipe title: ' + 
                        df['title'] + 
                        '. Ingredients: ' + 
                        df['ingredients'].apply(lambda x: '; '.join(x)) + 
                        '. Instructions: ' + 
                        df['instructions'])

df = (df.
      # remove adds
      pipe(remove_advertisement).
      # drop the picture_link column
      drop(['picture_link'], axis = 1).
      # give a num_words estimation
      assign(num_words = lambda d: d['full_text'].str.split().str.len()).
      # drop short recipes
      loc[lambda d: d['num_words'] > num_words_cutoff]
)

```

## Data Preprocessing

```{.python code-line-numbers="10-18|19-22|23-24|25-26|27-28"}
#| echo: true

# imports and json files parsing skipped
# see data.preprocess_data.combine_json_to_dataframe.py

# Combine the data from the three JSON files
data = {**fn_data, **epi_data, **ar_data}

# Convert the data to a dataframe
df = pd.DataFrame.from_dict(data, orient='index')

# Add a new column with the concatenated text
df['full_text'] = ('Recipe title: ' + 
                        df['title'] + 
                        '. Ingredients: ' + 
                        df['ingredients'].apply(lambda x: '; '.join(x)) + 
                        '. Instructions: ' + 
                        df['instructions'])

df = (df.
      # remove adds
      pipe(remove_advertisement).
      # drop the picture_link column
      drop(['picture_link'], axis = 1).
      # give a num_words estimation
      assign(num_words = lambda d: d['full_text'].str.split().str.len()).
      # drop short articles
      loc[lambda d: d['num_words'] > num_words_cutoff]
)

```

## Step 2 - Document Representations

::: {style="margin-top: 3em"}
- What is a `document`?
- What is a `representation`?
:::

## Document Representation Goal 

::: {style="margin-top: 3em;" }
Find `numerical representation` of the documents such that semantically `similar documents are close.`

And semantically `dissimilar documents - far. `
:::

## What does semantically similar mean?

![](/images/sem_similarity.png)

::: footer
Based on: Nils Reimers, Introduction to Dense Text Representations
:::

## Document representations

::: {style="margin-top: 3em"}
![](/images/doc_representations.png){fig-align="center"}
:::

## Document representations

::: {style="margin-top: 3em"}
![](/images/doc_reps_movies.png){fig-align="center"}
:::

## Sentence Transformers {chalkboard-buttons="true"}

::: {style="margin-top: 3em"}
A family of [language models](https://www.sbert.net/){preview-link="true"} finetuned specifically to produce document representations.

- Great for semantic search, clustering, classification...
- Work with multilanguage models as well. Images too!
- Build to work well with Huggingface and friends.

:::

## Sentence Transformers

![](/images/sbert.png)

::: footer
Source: Nils Reimers, Introduction to Dense Text Representations
:::


## SentenceTransformers - Triplet loss example

```python
{'set':
 {'query': 'What can I do to get better grades?',
  'pos': 
['How do I improve my grades?'],
  'neg': 
['Why do I get bad grades even though I study a lot?',
   'How can I get better grades in maths?',
   'How serious is forging high school grades?']
}
}
```

![](/images/triplet_loss.png)

::: footer
Nils Reimers, Introduction to Dense Text Representations
:::

## Loss functions

![](https://huggingface.co/blog/assets/95_training_st_models/datasets_table.png)

::: footer
Source: [Huggingface blog](https://huggingface.co/blog/how-to-train-sentence-transformers)
:::

## Sentence Transformers - Choices

::: {style="margin-top: 3em"}
- Many Sentence Transformers options
- Great docs
- [Pretrained Sentence Transformers](https://www.sbert.net/docs/pretrained_models.html){preview-link="true"}
:::

::: footer
\*Up to 512 tokens (ca. 400 words)
:::

## Choices - MTEB

Massive Text Embedding Benchmark ([MTEB](https://huggingface.co/spaces/mteb/leaderboard){preview-link="true" style="text-align: center"})

![](/images/mteb_2.png)

## Sentence Transformers API

::: {style="text-align: center; margin-top: 4em"}
```{python}
#| echo: true
from sentence_transformers import SentenceTransformer

# download model
vectoriser = SentenceTransformer("all-MiniLM-L12-v2")
# ensure that the model vectorises up to 512 tokens
vectoriser.max_seq_length = 512

docs = [rec for rec in recipe_data.full_text]
embeddings = vectoriser.encode(docs, show_progress_bar=True)
```
:::

## Sentence Transformers - Notes

::: {style="margin-top: 3em"}
- Up to max. 512 *tokens* (ca. 400 words). The rest of the text gets truncated.
  - Different `max_seq_length` defaults, can increase to 512.
- For longer docs - consider splitting (by paragraph, overlapping windows, etc...)
  - TF-IDF is a strong baseline.
:::

# How do you know if you've done a good job? {chalkboard-buttons="true"}

## Option 1 - Visualize

::: {style="margin-top: 3em"}
1. Reduce dimensions to 2 (UMAP seems to be the best option, TruncatedSVD for sparse data)
2. Create an interactive plot:
  - good options - altair, bokeh, plotly
:::

## Embeddings visualizations with Bulk

::: {style="margin-top: 1em"}
- [Bulk](https://github.com/koaning/bulk){preview-link="true"} - visualisation + initial data labelling in one.
- Works with images too!
- Steps:
  1. Reduce dimensions to 2 with UMAP.
  2. Add a text column.
  3. Save as a .csv
  4. `python -m bulk text data/bulk_st.csv`
:::

## Option 2 - Simple Semantic Search

```{.python}
def get_most_similar_doc(text:str, 
                         vectorized_corpus:np.array, 
                         original_data:pd.DataFrame, 
                         vectoriser:SentenceTransformer = vectoriser, 
                         top_n:int = 5):
    # encodes the input text
    new_doc_vector = vectoriser.encode(text)
    # calculates the cosine similarity between the new text embedding 
    # and all other vectors
    # cos sim from scikit-learn
    sim = cosine_similarity(X = vectorized_corpus, Y = new_doc_vector).flatten()
    # gets the indices of the top_n highest similarities
    argmax = np.argpartition(sim, -top_n)[-top_n:]
    # returns the original data filtered for the top results
    return(
        (original_data.
         iloc[argmax].
         # adds the cosine similarity as a column
         assign(similarity = sim[argmax]).
         loc[:, ['title', 'ingredients', 'similarity']].
         sort_values('similarity', ascending = False).
         reset_index(drop=True)
         )
    )
```

## Simple Semantic Search

```{.python}
creme_brulee_recipe = [
    """Ingredients:
- 2 cups heavy cream
- 1 vanilla bean, split and scraped or 1 tsp vanilla extract
- 5 egg yolks
- 1/2 cup granulated sugar, plus more for caramelizing

Instructions: ... truncated
"""
]

get_most_similar_doc(text = creme_brulee_recipe, 
                     vectorized_corpus = embeddings, 
                     original_data = recipe_data,
                     top_n=2)

# Returns    
## 1) Crème Brûlée                     - Cosine Similarity = 0.870680
## 2) Chocolate Sprinkled Creme Brulee - Cosine Similarity = 0.822773
```

# Further know your dataset

## Topic modelling with BERTopic

- [BERTopic](https://maartengr.github.io/BERTopic/index.html){preview-link="true"} - like a lego for topic modelling
- Embrace modularity
![](https://maartengr.github.io/BERTopic/algorithm/default.svg){width=80% height=80%}
- Fast, when you already have the embeddings

## BERTopic - simple

```{python}
#| echo: true

# Reduce stopwords in labels
vectoriser_model = CountVectorizer(stop_words="english")

topic_model = BERTopic(
    vectorizer_model=vectoriser_model
    )

topics, probs = topic_model.fit_transform(docs, embeddings)
```

## BERTopic - advanced

```{python}
#| echo: true

# Reduce stopwords in labels
vectoriser_model = CountVectorizer(stop_words="english")
# Customize clustering
hdbscan_model = HDBSCAN(min_cluster_size=10, 
                        min_samples = 1, # This to reduce outliers as much as possible
                        cluster_selection_epsilon = 0.1, # Reduce number of clusters
                        metric='euclidean', 
                        prediction_data=True)

# Improve label representation 
sentence_model = SentenceTransformer("all-MiniLM-L12-v2")
sentence_model.max_seq_length = 512
representation_model = MaximalMarginalRelevance(diversity=0.2)

topic_model = BERTopic(
    vectorizer_model=vectoriser_model,
    hdbscan_model = hdbscan_model,
    min_topic_size=20,
    n_gram_range=(1, 2),
    embedding_model=sentence_model, 
    representation_model=representation_model
    )

topics, probs = topic_model.fit_transform(docs, embeddings)
```


## Topic modelling with BERTopic

![](/images/bertopic_intertopic_distance.mov)

# Annotate your dataset

## Annotation

::: {style="margin-top: 1em"}
Any process of adding metadata tags to your text data can be called `annotation`.

Different examples of annotation:

- Text classification
- Named entities
- Entity linking
:::

## Some work - already done

::: {style="margin-top: 3em"}
- Use Bulk
- Use BERTopic
- Third option - annotation tools
:::

## Annotation Tools

1. Paid tools:
    - [Prodigy](https://prodi.gy/){preview-link="true"} (by Explosion (the SpaCy maintainers)) - supports `active learning`
    - [TagTog](https://www.tagtog.com/-plans){preview-link="true"}
    - [LightTag](https://www.lighttag.io/pricing){preview-link="true"}

2. Open Source tools:
    - [Argilla](https://docs.argilla.io/en/latest/index.html){preview-link="true"} - New and quite powerful.
    - [Doccano](https://doccano.github.io/doccano/){preview-link="true"}
    - [Pigeon](https://github.com/agermanidis/pigeon){preview-link="true"} - Simplest of all

## Pigeon - annotate in Jupyter

```{.python}
annotations = annotate(
  recipe_data.full_text.sample(100),
  # 1 - Very Easy, 2 - Kinda Easy, 3 - Moderate to Hard, 4 - Hard
  options=['1', '2', '3', '4'],
  # The below is needed just because the recipes are long and tough to see.
  display_fn=lambda x: pprint(x)
)
```

# Few-shot learning with SetFit

## Zero-shot vs. Few-shot learning

![](/images/zero-shot.png){.absolute top=70 left=0 width="45%" height="30%"}

![](/images/few-shot.png){.absolute bottom=0 right=0 width="45%" height="45%"}

::: footer
Source: GPT-3 paper https://arxiv.org/pdf/2005.14165.pdf
:::

## SetFit - How Does It Work?

::: {style="margin-top: 1em"}
1. Get some labelled data (10-15 examples per class are fine).
2. Fine-tune a SentenceTransformer with the labelled data.
  - Intuition - Think moving embeddings closer together, based on the labelled data.
3. Add a classification model (head) - can be any `scikit-learn` classifier, `pytorch` layers, etc...
4. Train the classification model with the fine-tuned SentenceTransformer.
:::

::: footer
See Huggingface's [blogpost](https://huggingface.co/blog/setfit) 
:::

## Few-shot learning with SetFit

::: {style="margin-top: 1em"}
```{.python}
# define the model
model_id = "sentence-transformers/all-MiniLM-L12-v2"
model = SetFitModel.from_pretrained(model_id)
model.model_body[0].max_seq_length = 512

# get the dataset - ca. 15 examples per 4 categories 
# 1 - easy to 4 - hard
annotated_df = pd.read_parquet("https://raw.githubusercontent.com/krumeto/oss_nlp_tools_demos/main/data/recipe_classes.parquet")
train_dataset = Dataset.from_pandas(annotated_df)

# Train the model
trainer = SetFitTrainer(
    model=model,
    train_dataset=train_dataset,
    loss_class=CosineSimilarityLoss,
    num_iterations=20,
    batch_size = 5, # Reduce the batch size due to memory issues
    column_mapping={"recipe": "text", "label": "label"},
)
trainer.train()

complicated_recipe = """Some complicated recipe"""

trainer.model.predict([complicated_recipe])
```
:::

# Deploy

## Streamlit - why do I love it?

::: {style="margin-top: 2em"}
1. Do your usual flow
2. Sprinkle some [streamlit](https://docs.streamlit.io/){preview-link="true"} calls
    
    - Love the docs & blog

3. Get your app. 
4. Stakeholders and teammates love it!
::: 

## Steamlit notes

::: {style="margin-top: 2em"}
Utilize caching - `@st.cache_resource` and `@st.cache_data` and wrap in functions

```{.python}
@st.cache_resource
def load_model():
    model = SentenceTransformer("all-MiniLM-L12-v2")
    model.max_seq_length = 512
    return(model)
```
:::

# Scale

## Indexes

Indexes have two main components:

- Storing docs + docs embeddings + metadata
- Scaled Search implementations:
  - KNN (also on GPU )
  - ANN (different algos like HNSW (Hierarchical Navigable Small World))
- See e.g. [this for index choices](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index){preview-link="true"}

## Vector Stores

- Indexing implementations:
  - ElasticSearch
  - FAISS (Facebook, but open source)
  - OpenSearch
  - Chroma
  - SimSity (small but cute)

- Some paid ones with free tiers:
  - Pinecone
  - Qdrant

## Example 1 - SimSity 

```{.python}
from simsity import create_index, load_index

# simsity requires the encoder class to have a `transform` method, thus this simple wrapper.
class Encoder:
    def __init__(self, model_name, max_seq_length = 512):
        self.model = SentenceTransformer(model_name)
        # Ensure we increase the max_seq_length to the maximum 512 to handle the long recipes
        self.model.max_seq_length = max_seq_length
    
    def transform(self, data:list):
        return self.model.encode(data)

encoder = Encoder(model_name="all-MiniLM-L12-v2")

# Populate the ANN vector index and use it. 
index = create_index([rec for rec in recipe_data.full_text], 
                     encoder,
                     path="../embeddings/"
                     )

test_recipe = """
Some recipe
"""
index.query(test_recipe, n=3)
```                     

## Example 2 - ChromaDB + LangChain

```{.python}
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L12-v2")
db = Chroma.from_texts(texts=[doc for doc in recipe_data.full_text], 
                           embedding=embeddings,
                           persist_directory="../embeddings/chromadb"
                           )
test_recipe = """
Some recipe
"""
results = db.similarity_search_with_score(test_recipe)

for doc in results[:3]:
    print("#"*50)
    print("Distance: ", doc[1])
    # First 100 chars of the doc
    print(doc[0].page_content[:100])
```

# A note on `LangChain`

## Summary

::: {style="margin-top: 2em"}
### Value in NLP can be created with Open Source Tools.
### Good embeddings open up opportunities.
### Search is much more accessible than chat. Might also be more powerful.
:::

# Q&A


# Additional Slides (could be messy, but kept for the curious)


## Old School - Term frequency

::: {layout="[[-1], [1], [-1]]"}
![](/images/term_freq.png){fig-align="center"}
:::

## Old School - Inverse Document Frequency

::: {layout="[[-1], [1], [-1]]"}
![](/images/idf.png){fig-align="center"}
:::

## Old School - TF-IDF

::: {layout="[[-3, 45,-19, 16, -19], [100]]" layout-valign="top"}
![](/images/term_freq.png){fig-align="right"}

![](/images/idf.png){fig-align="left"}

![](/images/tf-idf.png)
:::

## Old School - TF-IDF 

Using `sklearn`'s `TfidfVectorizer`:
```{python}
#| echo: true
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

vectoriser = TfidfVectorizer(
    stop_words='english', # default is without it, but this decreases the dictionary size significantly
    min_df = 2, # Ignore terms that have a document frequency strictly lower than the given threshold. When float, proportion of docs.
    max_df = 0.95, # ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).
    ngram_range=(1,2), # uni and bi-grams
    max_features=30_000, # unigrams are ca. 22K, get top 8000 bigrams
    dtype=np.float32 # Reduces the size of the resulting array without much quality sacrifice, default is float64
)

embeddings = vectoriser.fit_transform(recipe_data.full_text)
```

## Common Sparse Representation Issues

- Frequently mentioned:

  - Lexical gap: UK, United Kingdom, England
  - Word order not preserved

- Actual issues:

  - Requires a “vocabulary” of size (n_training docs, nr_tokens_retained) in memory e.g. (50 000, 100 000)
  - Huge vectors - tough for clustering tasks, store and scale.

## Dense Document Representations

::: {style="margin-top: 3em"}
- *`f`(Text)* -> *Representation^`n`*
- `n` dimensional representation
- Find function `f` such that semantically similar text is close
:::

::: footer
Source: Nils Reimers, Introduction to Dense Text Representations
:::


## Why do I strongly dislike closed source

::: incremental
- Not the spirit of ML.
- Not the spirit of SW.
- Danger of knowledge loss in the long term
:::

## Also

![](images/open-vs-closed-source.png){fig-align="center" width=110% height=110%}

::: footer
Source: https://twitter.com/julien_c/status/1648738233696632832/photo/1
:::

## What is a open source language model?

- [Definition](https://opensource.org/osd/)
- Open Source is a scale:
  - [Sentence Transformers](https://github.com/UKPLab/sentence-transformers/blob/master/LICENSE) are open source (Apache 2.0 License)
  - Huggingface's BLOOM is [NOT open source](https://bigscience.huggingface.co/blog/the-bigscience-rail-license)
  - But OpenAI's GPT-3.5/4 is as closed source as it gets
    - WTF is that - https://openai.com/policies

## Open Source LLM Landscape

![](images/os-recent-landscape.jpeg)

::: footer
Source: https://twitter.com/theaievangelist/status/1645809824314298368/photo/1
:::
